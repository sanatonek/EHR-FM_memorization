{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "from itertools import zip_longest\n",
    "from os.path import join\n",
    "from pprint import pprint\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import Mamba2ForCausalLM\n",
    "\n",
    "sns.set()\n",
    "\n",
    "SEED = 23\n",
    "ROOT = \"/path/to/project\"\n",
    "ID2LABEL_PATH = '/path/to/codes_dict'\n",
    "DATA_ROOT = '/path/to/data'\n",
    "FORECAST_INPUT_PATH = f'{DATA_ROOT}/prompt_data'\n",
    "FORECAST_OUTPUT_PATH = f'{DATA_ROOT}/forecast_data'\n",
    "SYPHILIS_PATH = f'{DATA_ROOT}/syphilis_data/*.parquet'\n",
    "MEMBERSHIP_INFERENCE_RESULTS_FILE = \"/h/afallah/odyssey/odyssey/membership_detection_results.parquet\"\n",
    "os.chdir(ROOT)\n",
    "\n",
    "AUGMENTED_TOKEN_MAP = {\n",
    "    \"type_ids\": \"type_tokens\",\n",
    "    \"ages\": \"age_tokens\",\n",
    "    \"time_stamps\": \"time_tokens\",\n",
    "    \"visit_orders\": \"position_tokens\",\n",
    "    \"visit_segments\": \"visit_tokens\",\n",
    "}\n",
    "\n",
    "ADDITIONAL_TOKEN_TYPES = list(AUGMENTED_TOKEN_MAP.keys())\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from odyssey.data.tokenizer import ConceptTokenizer\n",
    "from odyssey.data.dataset import PretrainDataset, PretrainDatasetDecoder, FinetuneDatasetDecoder\n",
    "from odyssey.models.model_utils import load_pretrain_data, load_finetune_data\n",
    "from odyssey.evals.prediction import load_pretrained_model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    \"\"\"Save the configuration arguments.\"\"\"\n",
    "    data_dir = \"odyssey/data/meds_data\"\n",
    "    vocab_dir = f\"{data_dir}/vocab\"\n",
    "    sequence_file = \"patient_sequences_2048.parquet\"\n",
    "    id_file = \"dataset_2048.pkl\"\n",
    "    valid_scheme = \"few_shot\"\n",
    "    num_finetune_patients = \"all\"\n",
    "    tasks = ['mortality_1month', 'readmission_1month', 'los_1week', 'c0', 'c1', 'c2']\n",
    "    checkpoint_dir = \"checkpoints/mamba2_pretrain_2048\"\n",
    "    model_path = f\"{checkpoint_dir}/best.ckpt\"\n",
    "    model_huggingface_dir = f\"{checkpoint_dir}/huggingface\"\n",
    "    max_len = 512\n",
    "    batch_size = 64\n",
    "    num_return_sequences = 100\n",
    "    chunk_size = 2000\n",
    "    max_patients = 50_000\n",
    "\n",
    "args.syphilis_sequences = glob.glob(SYPHILIS_PATH)\n",
    "args.forecast_inputs = glob.glob(os.path.join(FORECAST_INPUT_PATH, \"*.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plot Token Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_vocab = json.load(open(os.path.join(args.vocab_dir, \"metadata/meta_vocab.json\")))\n",
    "\n",
    "token2freq = {}\n",
    "for token, data in meta_vocab.items():\n",
    "    token2freq[token] = data['frequency']\n",
    "\n",
    "token2freq = dict(sorted(token2freq.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# ---\n",
    "\n",
    "random_df = pd.read_parquet(f\"{DATA_ROOT}/forecast_december_20/ehrmamba2_cls_prompt.parquet\")\n",
    "random_token2freq = {}\n",
    "for sequence in random_df['predicted_tokens']:\n",
    "    for token in sequence:\n",
    "        random_token2freq[token] = random_token2freq.get(token, 0) + 1\n",
    "\n",
    "random_token2freq = dict(sorted(random_token2freq.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens above threshold frequency and their frequencies from both distributions\n",
    "threshold = 25\n",
    "filtered_tokens = [t for t, f in token2freq.items() if f >= threshold and random_token2freq.get(t, 0) >= threshold]\n",
    "token_types = [t.split('//')[0].split('_')[0] for t in filtered_tokens]\n",
    "unique_types = list(set(token_types))\n",
    "\n",
    "# Custom high-contrast color palette (manually chosen for clarity)\n",
    "custom_palette = [\n",
    "    \"#1f77b4\",  # blue\n",
    "    \"#ff7f0e\",  # orange\n",
    "    \"#2ca02c\",  # green\n",
    "    \"#d62728\",  # red\n",
    "    \"#9467bd\",  # purple\n",
    "    \"#8c564b\",  # brown\n",
    "    \"#e377c2\",  # pink\n",
    "    \"#7f7f7f\",  # gray\n",
    "    \"#bcbd22\",  # olive\n",
    "    \"#17becf\",  # cyan\n",
    "]\n",
    "# If more types than colors, cycle through\n",
    "type_to_color = dict(zip(unique_types, itertools.cycle(custom_palette)))\n",
    "colors = [type_to_color[t] for t in token_types]\n",
    "\n",
    "original_freqs = [token2freq[t] for t in filtered_tokens]\n",
    "random_freqs = [random_token2freq[t] for t in filtered_tokens]\n",
    "\n",
    "# Create dataframe for plotting\n",
    "plot_df = pd.DataFrame({\n",
    "    'Original Frequency': original_freqs,\n",
    "    'Generated Frequency': random_freqs,\n",
    "    'Type': token_types\n",
    "})\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.xticks(fontsize=int(12 * 1.3 * 1.2))\n",
    "plt.yticks(fontsize=int(12 * 1.3 * 1.2))\n",
    "\n",
    "scatter = sns.scatterplot(\n",
    "    data=plot_df,\n",
    "    x='Original Frequency',\n",
    "    y='Generated Frequency',\n",
    "    hue='Type',\n",
    "    alpha=0.5,\n",
    "    palette=type_to_color\n",
    ")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title(\n",
    "    f'Token Frequency Distribution by Type (both freq >= {threshold:,})',\n",
    "    fontsize=int(16 * 1.3 * 1.2),\n",
    "    pad=30,\n",
    "    loc='center'  # Center the title\n",
    ")\n",
    "plt.xlabel('Original Frequency', fontsize=int(16 * 1.3 * 1.2))\n",
    "plt.ylabel('Generated Frequency', fontsize=int(16 * 1.3 * 1.2))\n",
    "\n",
    "# Make legend font larger and add background\n",
    "legend = plt.legend(fontsize=int(12 * 1.3 * 1.2), title='Type', title_fontsize=int(13 * 1.3 * 1.2), frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "frame.set_edgecolor('black')\n",
    "frame.set_alpha(0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens above threshold frequency and their frequencies\n",
    "threshold = 5\n",
    "filtered_tokens = [t for t, f in token2freq.items() if f >= threshold and random_token2freq.get(t, 0) >= threshold]\n",
    "token_types = [t.split('//')[0].split('_')[0] for t in filtered_tokens]\n",
    "unique_types = list(set(token_types))\n",
    "type_to_color = dict(zip(unique_types, sns.color_palette(\"husl\", len(unique_types))))\n",
    "colors = [type_to_color[t] for t in token_types]\n",
    "\n",
    "original_freqs = [token2freq[t] for t in filtered_tokens]\n",
    "random_freqs = [random_token2freq[t] for t in filtered_tokens]\n",
    "\n",
    "# Normalize frequencies\n",
    "total_original = sum(original_freqs)\n",
    "total_random = sum(random_freqs)\n",
    "original_freqs_norm = [f/total_original for f in original_freqs]\n",
    "random_freqs_norm = [f/total_random for f in random_freqs]\n",
    "\n",
    "# Create dataframe for plotting\n",
    "plot_df = pd.DataFrame({\n",
    "    'Token': range(len(filtered_tokens)),\n",
    "    'Original': original_freqs_norm,\n",
    "    'Generated': random_freqs_norm,\n",
    "    'Type': token_types\n",
    "}).melt(id_vars=['Token', 'Type'], var_name='Distribution', value_name='Frequency')\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Plot Original in black first\n",
    "original_data = plot_df[plot_df['Distribution'] == 'Original']\n",
    "sns.lineplot(data=original_data, x='Token', y='Frequency', color='black', \n",
    "             linewidth=2, label='Original')\n",
    "\n",
    "# Plot Generated with colors by type\n",
    "generated_data = plot_df[plot_df['Distribution'] == 'Generated']\n",
    "sns.lineplot(data=generated_data, x='Token', y='Frequency', hue='Type',\n",
    "             linewidth=0.7, palette=type_to_color)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title(f'Normalized Token Frequency Distribution by Type (both freq >= {threshold:,})')\n",
    "plt.xlabel('Token Rank')\n",
    "plt.ylabel('Normalized Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ConceptTokenizer(\n",
    "    data_dir=args.vocab_dir,\n",
    "    start_token=\"[BOS]\",\n",
    "    end_token=\"[EOS]\",\n",
    "    time_tokens=None,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "tokenizer.fit_on_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mamba2ForCausalLM.from_pretrained(\n",
    "    args.model_huggingface_dir,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Forecasting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_and_loader(data, tokenizer, max_len, batch_size, num_workers=4):\n",
    "    \"\"\"Create dataset and dataloader for inference\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing patient data\n",
    "        tokenizer: Tokenizer instance\n",
    "        max_len: Max sequence length to use\n",
    "        batch_size: Batch size for dataloader\n",
    "        num_workers: Number of workers for dataloader\n",
    "        \n",
    "    Returns:\n",
    "        dataset: PretrainDatasetDecoder instance\n",
    "        dataloader: DataLoader instance\n",
    "    \"\"\"\n",
    "    dataset = PretrainDatasetDecoder(\n",
    "        data=data,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len,\n",
    "        additional_token_types=None,\n",
    "        padding_side=\"right\",\n",
    "        return_attention_mask=False,\n",
    "        return_labels=False\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(\n",
    "    data_dir: str,\n",
    "    sequence_file: str,\n",
    "    id_file: str,\n",
    ") -> pd.DataFrame:\n",
    "    sequence_path = join(data_dir, sequence_file)\n",
    "    id_path = join(data_dir, id_file)\n",
    "\n",
    "    if not os.path.exists(sequence_path):\n",
    "        raise FileNotFoundError(f\"Sequence file not found: {sequence_path}\")\n",
    "\n",
    "    if not os.path.exists(id_path):\n",
    "        raise FileNotFoundError(f\"ID file not found: {id_path}\")\n",
    "\n",
    "    data = pl.read_parquet(sequence_path).to_pandas()\n",
    "    with open(id_path, \"rb\") as file:\n",
    "        patient_ids = pickle.load(file)\n",
    "\n",
    "    return data.loc[data[\"patient_id\"].isin(patient_ids[\"test\"])]\n",
    "\n",
    "# Load pretrain data\n",
    "pretrain = load_pretrain_data(\n",
    "    args.data_dir,\n",
    "    'patient_sequences/'+args.sequence_file,\n",
    "    'patient_id_dict/'+args.id_file,\n",
    ")\n",
    "test = load_test_data(\n",
    "    args.data_dir,\n",
    "    'patient_sequences/'+args.sequence_file,\n",
    "    'patient_id_dict/'+args.id_file,\n",
    ")\n",
    "\n",
    "# Or load a sample subset of the data\n",
    "# sample = train_dataset[1000]\n",
    "# sample['concept_ids'] = sample['concept_ids'][:26]\n",
    "# sample['attention_mask'] = sample['attention_mask'][:26]\n",
    "# sample = {k: v.unsqueeze(0).to(device) for k, v in sample.items()}\n",
    "# concept_ids = sample['concept_ids'].unsqueeze(0).to(device)\n",
    "# sample['concept_ids']\n",
    "\n",
    "# generated = model.generate(\n",
    "    # input_ids=[sample['concept_ids']],\n",
    "    # max_new_tokens=10,\n",
    "    # attention_mask=sample['attention_mask'],\n",
    "    # labels=sample['labels']\n",
    "# )\n",
    "\n",
    "# cutoff = 1\n",
    "# selected_dataset = pretrain.loc[pretrain['event_tokens'].transform(len) > 1]  #100\n",
    "# selected_dataset.loc[:, 'event_tokens'] = selected_dataset['event_tokens'].transform(lambda x: x[:cutoff])\n",
    "\n",
    "# random.seed(SEED)\n",
    "# patient_ids = selected_dataset['patient_id'].unique().tolist()\n",
    "# random.shuffle(patient_ids)\n",
    "\n",
    "# # patient_ids = patient_ids[:10_000]\n",
    "# selected_dataset = selected_dataset.loc[selected_dataset['patient_id'].isin(patient_ids)]\n",
    "# selected_dataset = selected_dataset.set_index('patient_id').loc[patient_ids].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dataset = pretrain.loc[pretrain[\"event_tokens\"].transform(len) > 100]\n",
    "\n",
    "# Option 1\n",
    "random.seed(SEED)\n",
    "patient_ids = selected_dataset['patient_id'].unique().tolist()\n",
    "random.shuffle(patient_ids)\n",
    "\n",
    "cutoff = 2\n",
    "patient_ids = patient_ids[:50_000]\n",
    "selected_dataset = selected_dataset.loc[selected_dataset['patient_id'].isin(patient_ids)]\n",
    "selected_dataset.loc[:, 'event_tokens'] = selected_dataset['event_tokens'].transform(lambda x: x[:cutoff])\n",
    "selected_dataset = selected_dataset.set_index('patient_id').loc[patient_ids].reset_index()\n",
    "\n",
    "# ---\n",
    "\n",
    "selected_dataset_test = test.loc[test[\"event_tokens\"].transform(len) > 100]\n",
    "\n",
    "# Option 1\n",
    "random.seed(SEED)\n",
    "patient_ids = selected_dataset_test['patient_id'].unique().tolist()\n",
    "random.shuffle(patient_ids)\n",
    "\n",
    "patient_ids = patient_ids[:10_000]\n",
    "selected_dataset_test = selected_dataset_test.loc[selected_dataset_test['patient_id'].isin(patient_ids)]\n",
    "selected_dataset_test = selected_dataset_test.set_index('patient_id').loc[patient_ids].reset_index()\n",
    "\n",
    "\n",
    "# Option 2\n",
    "# selected_dataset = selected_dataset[\n",
    "#     selected_dataset[\"event_tokens\"].apply(\n",
    "#         lambda x: any(\"DIAG\" in str(x[i]) for i in range(4, min(7, len(x))))\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# selected_dataset.loc[:, \"diag_loc\"] = selected_dataset[\"event_tokens\"].apply(\n",
    "#     lambda tokens: next(\n",
    "#         (i for i in range(len(tokens)) if \"DIAG\" in str(tokens[i]) and i > 3), None\n",
    "#     )\n",
    "# )\n",
    "# diag_locs = selected_dataset[\"diag_loc\"].unique()\n",
    "\n",
    "# for loc in diag_locs:\n",
    "#     subset = selected_dataset[selected_dataset[\"diag_loc\"] == loc].copy()\n",
    "\n",
    "#     subset[\"event_tokens\"] = subset.apply(\n",
    "#         lambda row: row[\"event_tokens\"][: int(row[\"diag_loc\"]) + 1], axis=1\n",
    "#     )\n",
    "\n",
    "#     filename = f\"prompt_diag_loc_{int(loc)}.parquet\"\n",
    "#     subset.to_parquet(filename, index=False)\n",
    "\n",
    "#     print(f\"Saved {len(subset)} rows to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Membership Inference Test**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MembershipDetector:\n",
    "    def __init__(self, k_percent: float = 20.0):\n",
    "        self.k_percent = k_percent\n",
    "\n",
    "    def get_batch_probs(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        input_ids: torch.Tensor,\n",
    "        device: torch.device\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get token probabilities for a batch of sequences efficiently\n",
    "        \n",
    "        Args:\n",
    "            model: The model\n",
    "            input_ids: Tensor of shape [batch_size, seq_len]\n",
    "            device: torch device\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len-1] containing probabilities\n",
    "            for each token in each sequence\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Process sequence positions in parallel for the batch\n",
    "            for pos in range(seq_len - 1):\n",
    "                # Get inputs up to current position for all sequences\n",
    "                input_slice = input_ids[:, :pos+1]\n",
    "                \n",
    "                # Get model outputs for entire batch\n",
    "                outputs = model(input_slice)\n",
    "                logits = outputs.logits  # [batch_size, pos+1, vocab_size]\n",
    "                \n",
    "                # Get probabilities for next token for all sequences\n",
    "                next_token_probs = torch.softmax(logits[:, -1, :], dim=-1)  # [batch_size, vocab_size]\n",
    "                \n",
    "                # Get probability of actual next tokens for batch\n",
    "                actual_next_tokens = input_ids[:, pos+1]  # [batch_size]\n",
    "                batch_probs = next_token_probs.gather(1, actual_next_tokens.unsqueeze(1)).squeeze(1)  # [batch_size]\n",
    "                \n",
    "                all_probs.append(batch_probs)\n",
    "        \n",
    "        # Stack probabilities for all positions\n",
    "        return torch.stack(all_probs, dim=1)  # [batch_size, seq_len-1]\n",
    "\n",
    "    def compute_batch_scores(self, token_probs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute MIN-K% PROB scores for a batch of sequences\n",
    "        \n",
    "        Args:\n",
    "            token_probs: Tensor of shape [batch_size, seq_len] \n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size] containing detection scores\n",
    "        \"\"\"\n",
    "        # Convert to numpy for operations\n",
    "        probs_np = token_probs.cpu().numpy()\n",
    "        batch_size, seq_len = probs_np.shape\n",
    "        \n",
    "        # Calculate number of tokens to select (k%)\n",
    "        k = max(1, int(seq_len * self.k_percent / 100))\n",
    "        \n",
    "        # Initialize scores array\n",
    "        scores = np.zeros(batch_size)\n",
    "        \n",
    "        # Process each sequence in batch\n",
    "        for i in range(batch_size):\n",
    "            seq_probs = probs_np[i]\n",
    "            log_probs = np.log(seq_probs + 1e-10)\n",
    "            \n",
    "            # Get indices of k tokens with lowest probabilities\n",
    "            lowest_k_indices = np.argpartition(seq_probs, k)[:k]\n",
    "            \n",
    "            # Calculate score for this sequence\n",
    "            scores[i] = np.mean(log_probs[lowest_k_indices])\n",
    "        \n",
    "        return torch.tensor(scores, device=token_probs.device)\n",
    "\n",
    "    def process_dataloader(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        device: torch.device,\n",
    "        is_member: bool\n",
    "    ) -> List[dict]:\n",
    "        \"\"\"Process entire dataloader in batches\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=f\"Processing {'member' if is_member else 'non-member'} data\")):\n",
    "            # Get batch of sequences\n",
    "            input_ids = batch[\"concept_ids\"].to(device)\n",
    "            \n",
    "            # Get probabilities for entire batch\n",
    "            batch_probs = self.get_batch_probs(model, input_ids, device)\n",
    "            \n",
    "            # Compute scores for batch\n",
    "            batch_scores = self.compute_batch_scores(batch_probs)\n",
    "            \n",
    "            # Store results\n",
    "            for seq_idx, score in enumerate(batch_scores):\n",
    "                results.append({\n",
    "                    'batch_idx': batch_idx,\n",
    "                    'seq_idx': seq_idx,\n",
    "                    'is_member': is_member,\n",
    "                    'detection_score': score.item(),\n",
    "                    'min_prob': torch.min(batch_probs[seq_idx]).item(),\n",
    "                    'mean_prob': torch.mean(batch_probs[seq_idx]).item()\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def detect_membership(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        member_dataloader: torch.utils.data.DataLoader,\n",
    "        nonmember_dataloader: torch.utils.data.DataLoader,\n",
    "        device: torch.device\n",
    "    ) -> Tuple[float, pd.DataFrame]:\n",
    "        \"\"\"Run detection on both dataloaders\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Process both dataloaders\n",
    "            member_results = self.process_dataloader(model, member_dataloader, device, True)\n",
    "            nonmember_results = self.process_dataloader(model, nonmember_dataloader, device, False)\n",
    "        \n",
    "        # Combine and analyze results\n",
    "        results_df = pd.DataFrame(member_results + nonmember_results)\n",
    "        auc = roc_auc_score(results_df['is_member'], results_df['detection_score'])\n",
    "        \n",
    "        return auc, results_df\n",
    "\n",
    "\n",
    "def run_membership_detection(\n",
    "    model,\n",
    "    member_dataloader,\n",
    "    nonmember_dataloader,\n",
    "    device,\n",
    "    k_percent=20.0,\n",
    "    results=None,\n",
    "    auc=0\n",
    ") -> Tuple[float, pd.DataFrame]:\n",
    "    \"\"\"Run membership detection with visualizations\"\"\"\n",
    "    if not all(results):\n",
    "        detector = MembershipDetector(k_percent=k_percent)\n",
    "        auc, results = detector.detect_membership(\n",
    "            model=model,\n",
    "            member_dataloader=member_dataloader,\n",
    "            nonmember_dataloader=nonmember_dataloader,\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nDetection Results:\")\n",
    "    print(f\"AUC Score: {auc:.3f}\")\n",
    "    \n",
    "    # Compute statistics\n",
    "    member_scores = results[results['is_member']]['detection_score']\n",
    "    non_member_scores = results[~results['is_member']]['detection_score']\n",
    "    \n",
    "    print(\"\\nMember sequences:\")\n",
    "    print(f\"Mean score: {member_scores.mean():.3f}\")\n",
    "    print(f\"Min score: {member_scores.min():.3f}\")\n",
    "    print(f\"Max score: {member_scores.max():.3f}\")\n",
    "    \n",
    "    print(\"\\nNon-member sequences:\")\n",
    "    print(f\"Mean score: {non_member_scores.mean():.3f}\")\n",
    "    print(f\"Min score: {non_member_scores.min():.3f}\")\n",
    "    print(f\"Max score: {non_member_scores.max():.3f}\")\n",
    "    \n",
    "    # Plot score distributions with custom font sizes and add padding between title and graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.histplot(data=results, x='detection_score', hue='is_member', bins=50, alpha=0.5)\n",
    "    plt.title(f'Distribution of Detection Scores (AUC = {auc:.3f})', fontsize=22, pad=30)\n",
    "    plt.xlabel('Detection Score', fontsize=18)\n",
    "    plt.ylabel('Count', fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    legend = ax.get_legend()\n",
    "    if legend is not None:\n",
    "        legend.set_frame_on(True)\n",
    "        legend.get_frame().set_facecolor('white')\n",
    "        for text in legend.get_texts():\n",
    "            text.set_fontsize(16)\n",
    "        legend.set_title(legend.get_title().get_text(), prop={'size': 16})\n",
    "        legend.set_bbox_to_anchor((0, 1))  # Move legend to top left\n",
    "        legend.set_loc(\"upper left\")\n",
    "    plt.show()\n",
    "    return auc, results\n",
    "\n",
    "\n",
    "\n",
    "# sequence_length = 512\n",
    "# max_sequences = 10_000\n",
    "\n",
    "# def get_random_sequence_slice(tokens, seq_len):\n",
    "#     if len(tokens) <= seq_len:\n",
    "#         return tokens\n",
    "#     start = random.randint(0, len(tokens) - seq_len)\n",
    "#     return tokens[start:start + seq_len]\n",
    "\n",
    "# # Randomly sample sequences\n",
    "# # Filter for sequences that are long enough\n",
    "# valid_pretrain = pretrain[pretrain['event_tokens'].str.len() >= sequence_length]\n",
    "# valid_test = test[test['event_tokens'].str.len() >= sequence_length]\n",
    "\n",
    "# random.seed(23)\n",
    "# member_indices = random.sample(range(len(valid_pretrain)), min(max_sequences, len(valid_pretrain)))\n",
    "# nonmember_indices = random.sample(range(len(valid_test)), min(max_sequences, len(valid_test)))\n",
    "\n",
    "# member_dataset = valid_pretrain.iloc[member_indices].copy()\n",
    "# nonmember_dataset = valid_test.iloc[nonmember_indices].copy()\n",
    "\n",
    "# member_dataset['event_tokens'] = member_dataset['event_tokens'].transform(\n",
    "#     lambda x: get_random_sequence_slice(x, sequence_length)\n",
    "# )\n",
    "# nonmember_dataset['event_tokens'] = nonmember_dataset['event_tokens'].transform(\n",
    "#     lambda x: get_random_sequence_slice(x, sequence_length)\n",
    "# )\n",
    "\n",
    "# _, member_dataloader = get_dataset_and_loader(\n",
    "#     data=member_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_len=sequence_length,\n",
    "#     batch_size=args.batch_size // 8,\n",
    "# )\n",
    "\n",
    "# _, nonmember_dataloader = get_dataset_and_loader(\n",
    "#     data=nonmember_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_len=sequence_length,\n",
    "#     batch_size=args.batch_size // 8,\n",
    "# )\n",
    "\n",
    "\n",
    "df = pd.read_parquet(MEMBERSHIP_INFERENCE_RESULTS_FILE)\n",
    "auc, results = run_membership_detection(\n",
    "    model=None,#model,\n",
    "    member_dataloader=None,#member_dataloader,\n",
    "    nonmember_dataloader=None,#nonmember_dataloader,\n",
    "    device=None,#device,\n",
    "    k_percent=20.0,\n",
    "    results=df,\n",
    "    auc=0.568\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_buffer_to_file(buffer: List[dict], output_file: str) -> None:\n",
    "    \"\"\"Write buffer contents to JSONL file.\"\"\"\n",
    "    mode = \"a\" if os.path.exists(output_file) else \"w\"\n",
    "    with open(output_file, mode) as f:\n",
    "        for item in buffer:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "def write_buffer_to_parquet(buffer: List[dict], temp_dir: str, chunk_idx: int) -> None:\n",
    "    \"\"\"Write buffer contents to parquet file.\"\"\"\n",
    "    df = pd.DataFrame(buffer)\n",
    "    chunk_path = os.path.join(temp_dir, f\"chunk_{chunk_idx}.parquet\")\n",
    "    df.to_parquet(chunk_path)\n",
    "\n",
    "\n",
    "def forecast(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: ConceptTokenizer,\n",
    "    dataloader: DataLoader,\n",
    "    patient_ids: List[str],\n",
    "    args: Any,\n",
    "    device: torch.device,\n",
    "    output_file: str,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.95,\n",
    "    do_sample: bool = True,\n",
    "    experiment: str = \"static_prompt\",\n",
    "):\n",
    "    \"\"\"Generate predictions using the model and save to JSONL file.\n",
    "\n",
    "    Args:\n",
    "        model: The model to use for generation\n",
    "        tokenizer: Tokenizer for decoding predictions\n",
    "        dataloader: DataLoader containing batches\n",
    "        patient_ids: List of patient IDs\n",
    "        args: Arguments containing batch_size, num_return_sequences, max_len\n",
    "        device: Device to run model on\n",
    "        output_file: Path to output JSONL file\n",
    "        temperature: Temperature for sampling (default: 1.0)\n",
    "        top_p: Top-p sampling parameter (default: 0.95)\n",
    "        do_sample: Whether to sample or use greedy decoding (default: True)\n",
    "        experiment: Experiment name (default: \"static_prompt\")\n",
    "    \"\"\"\n",
    "    buffer = []\n",
    "    buffer_size = args.batch_size * args.num_return_sequences * 10\n",
    "\n",
    "    # Create temp directory for chunks\n",
    "    temp_dir = os.path.join(\n",
    "        os.path.dirname(output_file),\n",
    "        f\"temp_{os.path.basename(output_file).split('.')[0]}\",\n",
    "    )\n",
    "    shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "    os.makedirs(temp_dir)\n",
    "    chunk_idx = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(\n",
    "            tqdm(\n",
    "                dataloader,\n",
    "                desc=f\"Generating predictions for {experiment}\",\n",
    "                total=len(dataloader),\n",
    "            )\n",
    "        ):\n",
    "\n",
    "            input_ids = batch[\"concept_ids\"].to(device)\n",
    "\n",
    "            # TODO: Investigage why this is needed\n",
    "            # input_ids = input_ids[:, :cutoff]\n",
    "            # input_ids = input_ids.reshape(-1, 1)\n",
    "\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=args.max_len,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                num_return_sequences=args.num_return_sequences,\n",
    "                do_sample=do_sample,\n",
    "                num_beams=1,\n",
    "                pad_token_id=tokenizer.get_pad_token_id(),\n",
    "                eos_token_id=tokenizer.get_eos_token_id(),\n",
    "                bos_token_id=tokenizer.get_class_token_id(),\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "            start_idx = batch_idx * args.batch_size\n",
    "            end_idx = min((batch_idx + 1) * args.batch_size, len(patient_ids))\n",
    "            batch_patient_ids = patient_ids[start_idx:end_idx]\n",
    "            batch_patient_ids = [\n",
    "                pid\n",
    "                for pid in batch_patient_ids\n",
    "                for _ in range(args.num_return_sequences)\n",
    "            ]\n",
    "\n",
    "            for i, (patient_id, sequence) in enumerate(zip(batch_patient_ids, outputs)):\n",
    "                sequence = sequence.detach().cpu().numpy()\n",
    "                sequence = sequence[: np.argmax(sequence == 0) or len(sequence)]\n",
    "                predicted_tokens = tokenizer.decode(sequence).split(\" \")\n",
    "\n",
    "                buffer.append(\n",
    "                    {\n",
    "                        \"split\": \"pretrain\",\n",
    "                        \"experiment\": experiment,\n",
    "                        \"patient_id\": patient_id,\n",
    "                        \"trajectory\": i % args.num_return_sequences,\n",
    "                        \"predicted_tokens\": predicted_tokens,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if len(buffer) >= buffer_size:\n",
    "                write_buffer_to_parquet(buffer, temp_dir, chunk_idx)\n",
    "                chunk_idx += 1\n",
    "                buffer = []\n",
    "\n",
    "        if buffer:\n",
    "            write_buffer_to_parquet(buffer, temp_dir, chunk_idx)\n",
    "\n",
    "    # Combine all chunks\n",
    "    chunk_files = sorted(glob.glob(os.path.join(temp_dir, \"chunk_*.parquet\")))\n",
    "    combined_df = pd.concat([pd.read_parquet(f) for f in chunk_files])\n",
    "    combined_df.to_parquet(output_file)\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "\n",
    "def membership_inference_batched(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: ConceptTokenizer,\n",
    "    dataloader: DataLoader,\n",
    "    patient_ids: List[str],\n",
    "    args: Any,\n",
    "    device: torch.device,\n",
    "    output_file: str,\n",
    "    mask_percent: float = 10.0,\n",
    "    n_runs: int = 100,\n",
    "    experiment: str = \"membership_inference\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform membership inference using efficient batching, masking tokens\n",
    "    and analyzing embedding variance.\n",
    "\n",
    "    Args:\n",
    "        model: The model to use for inference\n",
    "        tokenizer: Tokenizer for processing tokens\n",
    "        dataloader: DataLoader containing batches\n",
    "        patient_ids: List of patient IDs\n",
    "        args: Arguments containing batch_size, etc.\n",
    "        device: Device to run model on\n",
    "        output_file: Path to output parquet file\n",
    "        mask_percent: Percentage of tokens to mask (default: 10.0)\n",
    "        n_runs: Number of masking runs per sequence (default: 100)\n",
    "        experiment: Experiment name (default: \"membership_inference\")\n",
    "    \"\"\"\n",
    "    buffer = []\n",
    "    buffer_size = args.batch_size * 10\n",
    "\n",
    "    # Create temp directory for chunks\n",
    "    temp_dir = os.path.join(\n",
    "        os.path.dirname(output_file),\n",
    "        f\"temp_{os.path.basename(output_file).split('.')[0]}\",\n",
    "    )\n",
    "    shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "    os.makedirs(temp_dir)\n",
    "    chunk_idx = 0\n",
    "\n",
    "    # Set up random token choices for masking\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(\n",
    "            tqdm(\n",
    "                dataloader,\n",
    "                desc=f\"Running membership inference with {mask_percent}% masking\",\n",
    "                total=len(dataloader),\n",
    "            )\n",
    "        ):\n",
    "            input_ids = batch[\"concept_ids\"].to(device)\n",
    "            attention_mask = batch.get(\"attention_mask\", torch.ones_like(input_ids)).to(\n",
    "                device\n",
    "            )\n",
    "\n",
    "            batch_size, seq_length = input_ids.size()\n",
    "            start_idx = batch_idx * args.batch_size\n",
    "            end_idx = min((batch_idx + 1) * args.batch_size, len(patient_ids))\n",
    "            batch_patient_ids = patient_ids[start_idx:end_idx][:batch_size]\n",
    "\n",
    "            # Calculate actual sequence lengths\n",
    "            seq_lengths = torch.sum(attention_mask, dim=1).cpu().numpy().astype(int)\n",
    "\n",
    "            # Store embeddings for all sequences and runs\n",
    "            all_embeddings = [[] for _ in range(batch_size)]\n",
    "\n",
    "            # Run multiple passes with different masking patterns\n",
    "            for run in range(n_runs):\n",
    "                # Create a masked version of the input batch\n",
    "                masked_input = input_ids.clone()\n",
    "\n",
    "                # Apply masking for each sequence in the batch\n",
    "                for seq_idx in range(batch_size):\n",
    "                    seq_len = seq_lengths[seq_idx]\n",
    "\n",
    "                    # Number of tokens to mask for this sequence\n",
    "                    num_masked = max(1, int((mask_percent / 100) * seq_len))\n",
    "\n",
    "                    # Randomly select positions to mask (avoid first token)\n",
    "                    mask_indices = (\n",
    "                        np.random.choice(seq_len - 1, num_masked, replace=False) + 1\n",
    "                    )\n",
    "\n",
    "                    # Apply masking with random tokens\n",
    "                    for idx in mask_indices:\n",
    "                        random_token = torch.randint(\n",
    "                            10, min(vocab_size - 1, 5000), (1,)\n",
    "                        ).item()\n",
    "                        masked_input[seq_idx, idx] = random_token\n",
    "\n",
    "                # Forward pass through the model\n",
    "                outputs = model(\n",
    "                    masked_input,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=True,\n",
    "                )\n",
    "\n",
    "                # Extract hidden states from the last layer\n",
    "                hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "                # Extract final token embeddings for each sequence\n",
    "                for seq_idx in range(batch_size):\n",
    "                    final_token_pos = seq_lengths[seq_idx] - 1\n",
    "                    embedding = hidden_states[seq_idx, final_token_pos].cpu().numpy()\n",
    "                    all_embeddings[seq_idx].append(embedding)\n",
    "\n",
    "            # Compute statistics and store results\n",
    "            for i, patient_id in enumerate(batch_patient_ids):\n",
    "                if i >= len(all_embeddings):\n",
    "                    continue\n",
    "\n",
    "                # Stack embeddings for this sequence\n",
    "                embeddings = np.stack(all_embeddings[i])\n",
    "\n",
    "                # Compute variance across runs\n",
    "                var_embedding = np.var(embeddings, axis=0)\n",
    "\n",
    "                # Compute membership inference scores\n",
    "                avg_variance = float(np.mean(var_embedding))\n",
    "                max_variance = float(np.max(var_embedding))\n",
    "                total_variance = float(np.sum(var_embedding))\n",
    "\n",
    "                buffer.append(\n",
    "                    {\n",
    "                        \"patient_id\": patient_id,\n",
    "                        \"experiment\": experiment,\n",
    "                        \"mask_percent\": mask_percent,\n",
    "                        \"n_runs\": n_runs,\n",
    "                        \"avg_variance\": avg_variance,\n",
    "                        \"max_variance\": max_variance,\n",
    "                        \"total_variance\": total_variance,\n",
    "                        \"sequence_length\": int(seq_lengths[i]),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if len(buffer) >= buffer_size:\n",
    "                write_buffer_to_parquet(buffer, temp_dir, chunk_idx)\n",
    "                chunk_idx += 1\n",
    "                buffer = []\n",
    "\n",
    "    if buffer:\n",
    "        write_buffer_to_parquet(buffer, temp_dir, chunk_idx)\n",
    "\n",
    "    # Combine all chunks\n",
    "    chunk_files = sorted(glob.glob(os.path.join(temp_dir, \"chunk_*.parquet\")))\n",
    "    combined_df = pd.concat([pd.read_parquet(f) for f in chunk_files])\n",
    "    combined_df.to_parquet(output_file)\n",
    "    shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run membership inference on pretrain data\n",
    "membership_results_file = os.path.join(FORECAST_OUTPUT_PATH, \"membership_inference_pretrain.parquet\")\n",
    "\n",
    "# Use selected_dataset that was already defined in the notebook\n",
    "dataset, dataloader = get_dataset_and_loader(\n",
    "    data=selected_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=args.max_len,\n",
    "    batch_size=args.batch_size,\n",
    ")\n",
    "patient_ids = selected_dataset[\"patient_id\"].tolist()\n",
    "\n",
    "# Run membership inference\n",
    "print(\"Running membership inference on pretrain data...\")\n",
    "membership_inference_batched(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataloader=dataloader,\n",
    "    patient_ids=patient_ids,\n",
    "    args=args,\n",
    "    device=device,\n",
    "    output_file=membership_results_file,\n",
    "    mask_percent=10,\n",
    "    n_runs=100,\n",
    "    experiment=\"pretrain_membership\",\n",
    ")\n",
    "\n",
    "print(f\"Membership inference results saved to {membership_results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run membership inference on test data\n",
    "test_membership_results_file = os.path.join(FORECAST_OUTPUT_PATH, \"membership_inference_test.parquet\")\n",
    "\n",
    "# Create dataset and dataloader for test data using selected_dataset_test\n",
    "test_dataset, test_dataloader = get_dataset_and_loader(\n",
    "    data=selected_dataset_test,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=args.max_len,\n",
    "    batch_size=args.batch_size,\n",
    ")\n",
    "test_patient_ids = selected_dataset_test[\"patient_id\"].tolist()\n",
    "\n",
    "# Run membership inference on test data\n",
    "print(\"Running membership inference on test data...\")\n",
    "membership_inference_batched(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataloader=test_dataloader,\n",
    "    patient_ids=test_patient_ids,\n",
    "    args=args,\n",
    "    device=device,\n",
    "    output_file=test_membership_results_file,\n",
    "    mask_percent=10,\n",
    "    n_runs=100,\n",
    "    experiment=\"test_membership\",\n",
    ")\n",
    "\n",
    "print(f\"Test membership inference results saved to {test_membership_results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forecasting\n",
    "dataset, dataloader = get_dataset_and_loader(\n",
    "    data=selected_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=args.max_len,\n",
    "    batch_size=args.batch_size,\n",
    ")\n",
    "patient_ids = selected_dataset[\"patient_id\"].tolist()\n",
    "\n",
    "forecast(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataloader=dataloader,\n",
    "    patient_ids=patient_ids,\n",
    "    args=args,\n",
    "    device=device,\n",
    "    output_file=\"ehrmamba2_cls_age_april20.parquet\",\n",
    "    experiment=\"cls_age\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forecasting for each input file\n",
    "for input_file in tqdm(\n",
    "    args.forecast_inputs,\n",
    "    desc=\"Processing files\",\n",
    "    total=len(args.forecast_inputs),\n",
    "    leave=False,\n",
    "):\n",
    "    # Create output filename\n",
    "    print(f\"Processing {input_file}\")\n",
    "    \n",
    "    base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "    output_file = os.path.join(FORECAST_OUTPUT_PATH, f\"{base_name}_forecast.parquet\")\n",
    "\n",
    "    # Load dataset and dataloader\n",
    "    selected_dataset = pd.read_parquet(input_file)\n",
    "    # selected_dataset = selected_dataset.iloc[:args.max_patients]\n",
    "    selected_dataset = selected_dataset.rename(columns={\"event_tokens\": \"event_tokens\"})   # event_token\n",
    "    prompt_length = selected_dataset.event_tokens.transform(len).unique()[0]\n",
    "\n",
    "    dataset, dataloader = get_dataset_and_loader(\n",
    "        data=selected_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=prompt_length,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    patient_ids = selected_dataset[\"patient_id\"].tolist()\n",
    "    # patient_ids = list(range(len(selected_dataset)))\n",
    "\n",
    "    # Run forecast\n",
    "    forecast(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataloader=dataloader,\n",
    "        patient_ids=patient_ids,\n",
    "        args=args,\n",
    "        device=device,\n",
    "        output_file=str(output_file),\n",
    "        experiment=os.path.basename(input_file).split(\".\")[0],\n",
    "    )\n",
    "    print(f\"Finished processing {os.path.basename(input_file)} to {os.path.basename(output_file)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in args.forecast_inputs:\n",
    "\n",
    "    if \"10\" in input_file:\n",
    "        print(\"SKIPPING\")\n",
    "        continue\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "    forecast_file = os.path.join(FORECAST_OUTPUT_PATH, f\"{base_name}_forecast.parquet\")\n",
    "    \n",
    "    if os.path.exists(forecast_file):\n",
    "        # Load and prep dataframes\n",
    "        prompt_df = pd.read_parquet(input_file)\n",
    "        age_cols = [col for col in prompt_df.columns if 'age' in col.lower()]\n",
    "        prompt_df = prompt_df[['patient_id'] + age_cols]\n",
    "        forecast_df = pd.read_parquet(forecast_file)\n",
    "        \n",
    "        # Reset indexes and repeat prompt rows to match forecast\n",
    "        prompt_df = prompt_df.reset_index(drop=True)\n",
    "        prompt_df = pd.concat([prompt_df] * args.num_return_sequences).reset_index(drop=True)\n",
    "        forecast_df = forecast_df.reset_index(drop=True)\n",
    "        \n",
    "        # Simple concat since indexes now align\n",
    "        merged = pd.concat([forecast_df, prompt_df[age_cols]], axis=1)\n",
    "        merged.to_parquet(forecast_file.replace('.parquet', '_with_age.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generate Patient Representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Syphilis Data\n",
    "dfs = []\n",
    "for f in args.syphilis_sequences:\n",
    "\n",
    "    if \"10\" in f:\n",
    "        print(\"SKIPPING\")\n",
    "        continue\n",
    "\n",
    "    df = pl.read_parquet(f).to_pandas()\n",
    "    print(f\"\\nFile: {f}, Length: {len(df)}\")\n",
    "    display(df.head())\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence2target = {\n",
    "    f: \"event_token\" for f in args.syphilis_sequences\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_sequence, target_column in list(sequence2target.items()): #sequence2target.items():\n",
    "    print(\n",
    "        f\"\\nPatient Sequence: {patient_sequence.split('/')[-1]}, Target Column: {target_column}\\n\"\n",
    "    )\n",
    "\n",
    "    # Create temp directory next to input parquet\n",
    "    temp_dir = os.path.join(\n",
    "        os.path.dirname(patient_sequence),\n",
    "        f\"temp_{os.path.basename(patient_sequence).split('.')[0]}\",\n",
    "    )\n",
    "    shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "    os.makedirs(temp_dir)\n",
    "\n",
    "    # Load and prep dataset\n",
    "    df = pl.read_parquet(patient_sequence).to_pandas()\n",
    "    df = df.rename(columns={target_column: \"event_tokens\"})\n",
    "\n",
    "    # Create dataset and dataloader (unchanged)\n",
    "    dataset = PretrainDatasetDecoder(\n",
    "        data=df,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=args.max_len,\n",
    "        additional_token_types=None,\n",
    "        padding_side=\"right\",\n",
    "        return_attention_mask=True,\n",
    "        return_labels=False,\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    # Process in chunks\n",
    "    buffer = []\n",
    "    chunk_idx = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(\n",
    "            tqdm(\n",
    "                dataloader,\n",
    "                desc=f\"Generating embeddings for {patient_sequence.split('/')[-1]}\",\n",
    "                total=len(dataloader),\n",
    "            )\n",
    "        ):\n",
    "            # Get embeddings for batch\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"concept_ids\"].to(device),\n",
    "                attention_mask=batch[\"attention_mask\"].to(device),\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "            # Process each sequence\n",
    "            for i, seq in enumerate(batch[\"concept_ids\"]):\n",
    "                non_pad_pos = (seq != tokenizer.get_pad_token_id()).nonzero()[-1][0] # -1\n",
    "                emb = hidden_states[i, non_pad_pos].cpu().numpy()\n",
    "                buffer.append(emb)\n",
    "\n",
    "            # Write buffer when full\n",
    "            if len(buffer) >= args.chunk_size or batch_idx == len(dataloader) - 1:\n",
    "                start_idx = chunk_idx * args.chunk_size\n",
    "                end_idx = start_idx + len(buffer)\n",
    "\n",
    "                chunk_df = df.iloc[start_idx:end_idx].copy()\n",
    "                chunk_df[f\"{target_column}_embeddings\"] = buffer\n",
    "                chunk_df = chunk_df.rename(columns={\"event_tokens\": target_column})\n",
    "\n",
    "                chunk_path = os.path.join(temp_dir, f\"chunk_{chunk_idx}.parquet\")\n",
    "                pl.from_pandas(chunk_df).write_parquet(chunk_path)\n",
    "\n",
    "                buffer = []\n",
    "                chunk_idx += 1\n",
    "\n",
    "    # Combine chunks and save final output\n",
    "    chunk_files = sorted(glob.glob(os.path.join(temp_dir, \"chunk_*.parquet\")))\n",
    "    combined_df = pl.concat([pl.read_parquet(f) for f in chunk_files])\n",
    "    combined_df.write_parquet(patient_sequence.split('/')[-1])  # CHANGE LATER ON!\n",
    "\n",
    "    # Cleanup\n",
    "    shutil.rmtree(temp_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
